{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(383218, 4415, 12565)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec = pd.read_csv(\"recommend-output-all-repo-top-500.csv\")\n",
    "\n",
    "to_lib2cnt = {to_lib: len(rows) for to_lib, rows in rec.groupby(by=\"toLib\")}\n",
    "from_lib2cnt = {from_lib: len(rows) for from_lib, rows in rec.groupby(by=\"fromLib\")}\n",
    "rec[\"confTeyton\"] = (rec[\"ruleCountSameCommit\"] / np.maximum(\n",
    "    rec[\"toLib\"].apply(lambda x: to_lib2cnt[x]),\n",
    "    rec[\"fromLib\"].apply(lambda x: from_lib2cnt[x])\n",
    ")).fillna(0)\n",
    "\n",
    "migrations = pd.read_excel(\"manual/extended-migrations-annotated.xlsx\")\n",
    "rules = set(zip(migrations[\"fromLib\"], migrations[\"toLib\"]))\n",
    "len(rec), len(rules), len(migrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4273, 611)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "migrations_true = migrations[migrations[\"isTrue\"]]\n",
    "rules_confirmed = set(zip(migrations_true[\"fromLib\"], migrations_true[\"toLib\"]))\n",
    "len(migrations_true), len(rules_confirmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(785, 1313)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(migrations_true[\"repoName\"])), len(set(migrations[\"repoName\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1233"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(migrations_true[\"startCommit\"]) | set(migrations_true[\"endCommit\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "231"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(migrations_true[\"fromLib\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2617"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec = rec[rec[\"fromLib\"].isin(migrations_true[\"fromLib\"])]\n",
    "# rec = rec[rec[\"toLib\"].apply(lambda x: \"infinispan\" not in x)]\n",
    "rec_filtered = rec[rec[[\"fromLib\",\"toLib\"]].apply(lambda x: (x[0], x[1]) in rules, axis=1)].copy()\n",
    "len(rec_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(method, rules, possible_rules, confirmed_rules):\n",
    "    top_k = 20\n",
    "    top_rules = [list() for x in range(0, top_k)]\n",
    "    ndcg_possible_at_k = [list() for x in range(0, top_k)]\n",
    "    ndcg_confirmed_at_k = [list() for x in range(0, top_k)]\n",
    "    from_lib_set = set(x for x, y in confirmed_rules)\n",
    "    reciprocal_ranks_confirmed = {}\n",
    "    reciprocal_ranks_possible = {}\n",
    "    for from_lib, candidates in rules.groupby(by=\"fromLib\"):\n",
    "        if from_lib not in from_lib_set:\n",
    "            continue\n",
    "        this_rules = [(from_lib, to_lib) for to_lib in candidates[\"toLib\"]]\n",
    "        this_possible_rules = [(from_lib, to_lib) for from_lib, to_lib in this_rules if (from_lib, to_lib) in possible_rules]\n",
    "        this_confirmed_rules = [(from_lib, to_lib) for from_lib, to_lib in this_rules if (from_lib, to_lib) in confirmed_rules]\n",
    "        last_k, last_ndcg = 0, 0\n",
    "        for k, (from_lib, to_lib) in enumerate(this_rules):\n",
    "            if k >= top_k:\n",
    "                continue\n",
    "            last_k = k\n",
    "            top_rules[k].append((from_lib, to_lib))\n",
    "            if (from_lib, to_lib) in possible_rules and from_lib not in reciprocal_ranks_possible:\n",
    "                reciprocal_ranks_possible[from_lib] = 1 / (k + 1)\n",
    "            if (from_lib, to_lib) in confirmed_rules and from_lib not in reciprocal_ranks_confirmed:\n",
    "                reciprocal_ranks_confirmed[from_lib] = 1 / (k + 1)\n",
    "            dcg_p = sum(int((from_lib, to_lib) in possible_rules) / np.log2(i+2) for i, (from_lib, to_lib) in enumerate(this_rules[0:k+1]))\n",
    "            idcg_p = sum(1 / np.log2(i+2) for i in range(0, min(k + 1, len(this_possible_rules))))\n",
    "            if idcg_p == 0:\n",
    "                ndcg_possible_at_k[k].append(0)\n",
    "            else:\n",
    "                ndcg_possible_at_k[k].append(dcg_p / idcg_p)\n",
    "            dcg_c = sum(int((from_lib, to_lib) in confirmed_rules) / np.log2(i+2) for i, (from_lib, to_lib) in enumerate(this_rules[0:k+1]))\n",
    "            idcg_c = sum(1 / np.log2(i+2) for i in range(0, min(k + 1, len(this_confirmed_rules))))\n",
    "            if idcg_c == 0:\n",
    "                ndcg_confirmed_at_k[k].append(0)\n",
    "                last_ndcg = 0\n",
    "            else:\n",
    "                ndcg_confirmed_at_k[k].append(dcg_c / idcg_c)\n",
    "                last_ndcg = dcg_c / idcg_c\n",
    "        for k in range(last_k + 1, top_k):\n",
    "            ndcg_confirmed_at_k[k].append(last_ndcg)\n",
    "        if from_lib not in reciprocal_ranks_possible:\n",
    "            reciprocal_ranks_possible[from_lib] = 0\n",
    "        if from_lib not in reciprocal_ranks_confirmed:\n",
    "            reciprocal_ranks_confirmed[from_lib] = 0\n",
    "            \n",
    "    for k in range(1, top_k):\n",
    "        top_rules[k] += top_rules[k - 1] \n",
    "    result = {\n",
    "        \"Name\": method,\n",
    "        \"FromLibCount\": len(from_lib_set & set(rules[\"fromLib\"])),\n",
    "        \"MRR-C\": np.mean(list(reciprocal_ranks_confirmed.values())),\n",
    "        \"MRR-P\": np.mean(list(reciprocal_ranks_possible.values())),\n",
    "        \"Precision-C@k\": [],\n",
    "        \"Precision-P@k\": [],\n",
    "        \"Recall-C@k\": [],\n",
    "        \"Recall-P@k\": [],\n",
    "        \"NDCG-C@k\": [],\n",
    "        \"NDCG-P@k\": [],\n",
    "    }\n",
    "    for k in range(0, top_k):\n",
    "        precision = len([x for x in top_rules[k] if x in confirmed_rules]) / len(top_rules[k])\n",
    "        recall = len([x for x in top_rules[k] if x in confirmed_rules]) / len(confirmed_rules)\n",
    "        precision_possible = len([x for x in top_rules[k] if x in possible_rules]) / len(top_rules[k])\n",
    "        recall_possible = len([x for x in top_rules[k] if x in possible_rules]) / len(possible_rules)\n",
    "        result[\"Precision-C@k\"].append(precision)\n",
    "        result[\"Precision-P@k\"].append(precision_possible)\n",
    "        result[\"Recall-C@k\"].append(recall)\n",
    "        result[\"Recall-P@k\"].append(recall_possible)\n",
    "        if len(ndcg_confirmed_at_k[k]) > 0:\n",
    "            result[\"NDCG-C@k\"].append(np.mean(ndcg_confirmed_at_k[k]))\n",
    "        else:\n",
    "            result[\"NDCG-C@k\"].append(0)\n",
    "        if len(ndcg_possible_at_k[k]) > 0:\n",
    "            result[\"NDCG-P@k\"].append(np.mean(ndcg_possible_at_k[k]))\n",
    "        else:\n",
    "            result[\"NDCG-P@k\"].append(0)\n",
    "    return result\n",
    "def print_evaluation_result(result):\n",
    "    print(\"Result of {} on {} Library Queries:\".format(result[\"Name\"], result[\"FromLibCount\"]))\n",
    "    print(\"MRR-C/P = {}/{}\".format(result[\"MRR-C\"], result[\"MRR-P\"]))\n",
    "    for k in range(0, len(result[\"Precision-C@k\"])):\n",
    "        if k + 1 > 10 and (k + 1) % 10 != 0:\n",
    "            continue\n",
    "        print(\"Top {:3}: Precision = {:0.4f}, Recall = {:0.4f}, NDCG = {:0.4f}\"\n",
    "              .format(k + 1, result[\"Precision-C@k\"][k], result[\"Recall-C@k\"][k], result[\"NDCG-C@k\"][k]))\n",
    "def print_one_line_evaluation_result(result):\n",
    "    print(\"{:30}: Precision@1 = {:0.4f}, MRR = {:0.4f}, Recall@5 = {:0.4f}, Recall@10 = {:0.4f}, \"\n",
    "         \"Recall@20 = {:0.4f}, NDCG@10 = {:0.4f}\".format(\n",
    "             result[\"Name\"],\n",
    "             result[\"Precision-C@k\"][0],\n",
    "             result[\"MRR-C\"],\n",
    "             result[\"Recall-C@k\"][4],\n",
    "             result[\"Recall-C@k\"][9],\n",
    "             result[\"Recall-C@k\"][19],\n",
    "             result[\"NDCG-C@k\"][9]\n",
    "         ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of Our Method on 230 Library Queries:\n",
      "MRR-C/P = 0.7880833013249893/0.7880833013249893\n",
      "Top   1: Precision = 0.6783, Recall = 0.2553, NDCG = 0.6783\n",
      "Top   2: Precision = 0.5413, Recall = 0.4075, NDCG = 0.6856\n",
      "Top   3: Precision = 0.4507, Recall = 0.5090, NDCG = 0.6962\n",
      "Top   4: Precision = 0.3891, Recall = 0.5859, NDCG = 0.7133\n",
      "Top   5: Precision = 0.3391, Recall = 0.6383, NDCG = 0.7235\n",
      "Top   6: Precision = 0.3080, Recall = 0.6956, NDCG = 0.7372\n",
      "Top   7: Precision = 0.2789, Recall = 0.7349, NDCG = 0.7467\n",
      "Top   8: Precision = 0.2565, Recall = 0.7725, NDCG = 0.7583\n",
      "Top   9: Precision = 0.2382, Recall = 0.8069, NDCG = 0.7675\n",
      "Top  10: Precision = 0.2191, Recall = 0.8249, NDCG = 0.7702\n",
      "Top  20: Precision = 0.1326, Recall = 0.9984, NDCG = 0.8048\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(\"Our Method\", rec, rules_confirmed, rules_confirmed)\n",
    "print_evaluation_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of Our Method on 230 Library Queries:\n",
      "MRR-C/P = 0.8426552795031056/0.8426552795031056\n",
      "Top   1: Precision = 0.7565, Recall = 0.2848, NDCG = 0.7565\n",
      "Top   2: Precision = 0.5996, Recall = 0.4435, NDCG = 0.7516\n",
      "Top   3: Precision = 0.5052, Recall = 0.5548, NDCG = 0.7628\n",
      "Top   4: Precision = 0.4381, Recall = 0.6318, NDCG = 0.7748\n",
      "Top   5: Precision = 0.3942, Recall = 0.6956, NDCG = 0.7875\n",
      "Top   6: Precision = 0.3610, Recall = 0.7480, NDCG = 0.8000\n",
      "Top   7: Precision = 0.3338, Recall = 0.7856, NDCG = 0.8085\n",
      "Top   8: Precision = 0.3132, Recall = 0.8216, NDCG = 0.8170\n",
      "Top   9: Precision = 0.2978, Recall = 0.8543, NDCG = 0.8251\n",
      "Top  10: Precision = 0.2831, Recall = 0.8756, NDCG = 0.8284\n",
      "Top  20: Precision = 0.2331, Recall = 0.9984, NDCG = 0.8498\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(\"Our Method\", rec_filtered, rules_confirmed, rules_confirmed)\n",
    "print_evaluation_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teyton et al. 2013            : Precision@1 = 0.6174, MRR = 0.7066, Recall@5 = 0.5270, Recall@10 = 0.6710, Recall@20 = 0.8380, NDCG@10 = 0.6468\n",
      "Teyton et al. 2013'           : Precision@1 = 0.6035, MRR = 0.6985, Recall@5 = 0.5172, Recall@10 = 0.6628, Recall@20 = 0.8020, NDCG@10 = 0.6653\n",
      "Teyton et al. 2013''          : Precision@1 = 0.8148, MRR = 0.8410, Recall@5 = 0.2209, Recall@10 = 0.2226, Recall@20 = 0.2226, NDCG@10 = 0.8475\n",
      "Alrubaye et al. 2019          : Precision@1 = 0.9143, MRR = 0.9143, Recall@5 = 0.0540, Recall@10 = 0.0540, Recall@20 = 0.0540, NDCG@10 = 0.9143\n",
      "Our Approach                  : Precision@1 = 0.6870, MRR = 0.7918, Recall@5 = 0.6514, Recall@10 = 0.8314, Recall@20 = 0.9918, NDCG@10 = 0.7770\n"
     ]
    }
   ],
   "source": [
    "def teyton_2013(rules, t):\n",
    "    return rules[rules[\"confTeyton\"]>=t].sample(frac=1).sort_values(by=[\"fromLib\", \"confTeyton\"], ascending=[True, False])\n",
    "def method(rules, a, b, c, d):\n",
    "    rules[\"confidence\"] = (\n",
    "        rules[\"ruleFreqSameCommit\"] ** a\n",
    "        * np.maximum(0.1, rules[\"apiSupport\"]) ** b\n",
    "        * rules[\"commitDistance\"] ** c\n",
    "        * np.log2(rules[\"possibleCommitCount\"] + 1) ** d\n",
    "    )\n",
    "    return rules.sample(frac=1).sort_values(by=[\"fromLib\", \"confidence\"], ascending=[True, False])\n",
    "def our_method(rules):\n",
    "    rules[\"confidence\"] = (\n",
    "        rules[\"ruleFreqSameCommit\"]\n",
    "        * np.maximum(0.1, rules[\"apiSupport\"])\n",
    "        * rules[\"commitDistance\"]\n",
    "        * np.log2(rules[\"possibleCommitCount\"] + 1)\n",
    "    )\n",
    "    return rules.sample(frac=1).sort_values(by=[\"fromLib\", \"confidence\"], ascending=[True, False])\n",
    "def parallel_worker(name, func, params):\n",
    "    return evaluate(name, func(rec, *params), rules_confirmed, rules_confirmed)\n",
    "methods = [\n",
    "    (\"Teyton et al. 2013\", teyton_2013, (0,)),\n",
    "    (\"Teyton et al. 2013'\", teyton_2013, (0.002,)),\n",
    "    (\"Teyton et al. 2013''\", teyton_2013, (0.015,)),\n",
    "    (\"Alrubaye et al. 2019\", alrubaye_2019, ()),\n",
    "    (\"Our Approach\", our_method, ())\n",
    "]\n",
    "pool = multiprocessing.Pool(12)\n",
    "results = pool.starmap(parallel_worker, methods)\n",
    "pool.close()\n",
    "pool.join()\n",
    "for result in results:\n",
    "    print_one_line_evaluation_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of Alrubaye et al. 2019 on 35 Library Queries:\n",
      "MRR-C/P = 0.9142857142857143/0.9142857142857143\n",
      "Top   1: Precision = 0.9143, Recall = 0.0524, NDCG = 0.9143\n",
      "Top   2: Precision = 0.8919, Recall = 0.0540, NDCG = 0.9143\n",
      "Top   3: Precision = 0.8919, Recall = 0.0540, NDCG = 0.9143\n",
      "Top   4: Precision = 0.8919, Recall = 0.0540, NDCG = 0.9143\n",
      "Top   5: Precision = 0.8919, Recall = 0.0540, NDCG = 0.9143\n",
      "Top   6: Precision = 0.8919, Recall = 0.0540, NDCG = 0.9143\n",
      "Top   7: Precision = 0.8919, Recall = 0.0540, NDCG = 0.9143\n",
      "Top   8: Precision = 0.8919, Recall = 0.0540, NDCG = 0.9143\n",
      "Top   9: Precision = 0.8919, Recall = 0.0540, NDCG = 0.9143\n",
      "Top  10: Precision = 0.8919, Recall = 0.0540, NDCG = 0.9143\n",
      "Top  20: Precision = 0.8919, Recall = 0.0540, NDCG = 0.9143\n"
     ]
    }
   ],
   "source": [
    "def alrubaye_2019(rules):\n",
    "    return rules[\n",
    "        (rules[\"methodChangeCount\"] > 0) & (rules[\"ruleFreqSameCommit\"] >= 0.6)\n",
    "    ].sample(frac=1).sort_values(by=[\"fromLib\", \"ruleFreqSameCommit\"], ascending=[True, False])\n",
    "result = evaluate(\"Alrubaye et al. 2019\", alrubaye_2019(rec), rules_confirmed, rules_confirmed)\n",
    "print_evaluation_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of Teyton et al. 2019 on 230 Library Queries:\n",
      "MRR-C/P = 0.6936973683500808/0.6936973683500808\n",
      "Top   1: Precision = 0.5913, Recall = 0.2226, NDCG = 0.5913\n",
      "Top   2: Precision = 0.4565, Recall = 0.3437, NDCG = 0.5750\n",
      "Top   3: Precision = 0.3681, Recall = 0.4157, NDCG = 0.5722\n",
      "Top   4: Precision = 0.3239, Recall = 0.4877, NDCG = 0.5927\n",
      "Top   5: Precision = 0.2809, Recall = 0.5286, NDCG = 0.6023\n",
      "Top   6: Precision = 0.2493, Recall = 0.5630, NDCG = 0.6116\n",
      "Top   7: Precision = 0.2242, Recall = 0.5908, NDCG = 0.6179\n",
      "Top   8: Precision = 0.2065, Recall = 0.6219, NDCG = 0.6263\n",
      "Top   9: Precision = 0.1903, Recall = 0.6448, NDCG = 0.6326\n",
      "Top  10: Precision = 0.1787, Recall = 0.6727, NDCG = 0.6391\n",
      "Top  20: Precision = 0.1133, Recall = 0.8527, NDCG = 0.6845\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(\"Teyton et al. 2019\", teyton_2013(rec, 0.0), rules_confirmed, rules_confirmed)\n",
    "print_evaluation_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of Teyton et al. 2019 on 227 Library Queries:\n",
      "MRR-C/P = 0.698466763865831/0.698466763865831\n",
      "Top   1: Precision = 0.6035, Recall = 0.2242, NDCG = 0.6035\n",
      "Top   2: Precision = 0.4578, Recall = 0.3372, NDCG = 0.5819\n",
      "Top   3: Precision = 0.3868, Recall = 0.4223, NDCG = 0.5966\n",
      "Top   4: Precision = 0.3349, Recall = 0.4812, NDCG = 0.6119\n",
      "Top   5: Precision = 0.2921, Recall = 0.5172, NDCG = 0.6197\n",
      "Top   6: Precision = 0.2670, Recall = 0.5581, NDCG = 0.6348\n",
      "Top   7: Precision = 0.2444, Recall = 0.5859, NDCG = 0.6442\n",
      "Top   8: Precision = 0.2281, Recall = 0.6137, NDCG = 0.6518\n",
      "Top   9: Precision = 0.2162, Recall = 0.6416, NDCG = 0.6597\n",
      "Top  10: Precision = 0.2052, Recall = 0.6628, NDCG = 0.6653\n",
      "Top  20: Precision = 0.1487, Recall = 0.8020, NDCG = 0.7003\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(\"Teyton et al. 2019\", teyton_2013(rec, 0.002), rules_confirmed, rules_confirmed)\n",
    "print_evaluation_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of Teyton et al. 2019 on 108 Library Queries:\n",
      "MRR-C/P = 0.8425925925925926/0.8425925925925926\n",
      "Top   1: Precision = 0.8148, Recall = 0.1440, NDCG = 0.8148\n",
      "Top   2: Precision = 0.6894, Recall = 0.1817, NDCG = 0.8323\n",
      "Top   3: Precision = 0.6392, Recall = 0.2029, NDCG = 0.8409\n",
      "Top   4: Precision = 0.6209, Recall = 0.2144, NDCG = 0.8463\n",
      "Top   5: Precision = 0.6027, Recall = 0.2209, NDCG = 0.8479\n",
      "Top   6: Precision = 0.5939, Recall = 0.2226, NDCG = 0.8490\n",
      "Top   7: Precision = 0.5862, Recall = 0.2226, NDCG = 0.8490\n",
      "Top   8: Precision = 0.5837, Recall = 0.2226, NDCG = 0.8490\n",
      "Top   9: Precision = 0.5837, Recall = 0.2226, NDCG = 0.8490\n",
      "Top  10: Precision = 0.5837, Recall = 0.2226, NDCG = 0.8490\n",
      "Top  20: Precision = 0.5837, Recall = 0.2226, NDCG = 0.8490\n"
     ]
    }
   ],
   "source": [
    "result = evaluate(\"Teyton et al. 2019\", teyton_2013(rec, 0.015), rules_confirmed, rules_confirmed)\n",
    "print_evaluation_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
